{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR NORMAL IMAGES\n",
    "# positive_path = 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_sampled_positive\\\\'\n",
    "# negative_path = 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_sampled_negative\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR NORMAL W/AUG IMAGES\n",
    "positive_path = 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\1200aug_equalized_sampled_positive\\\\'\n",
    "negative_path = 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\1200aug_equalized_sampled_negative\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR MASKED IMAGES\n",
    "# positive_path = 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\masked_images\\\\positive\\\\'\n",
    "# negative_path = 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\masked_images\\\\negative\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(negative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_data\\\\working\\\\train')\n",
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_data/working/test')\n",
    "\n",
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_data/working/train/positive')\n",
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_data/working/test/positive')\n",
    "\n",
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_data/working/train/negative')\n",
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_data/working/test/negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_len = int(np.floor(len(os.listdir(positive_path))*0.8))\n",
    "positive_len = len(os.listdir(positive_path))\n",
    "\n",
    "negative_train_len = int(np.floor(len(os.listdir(negative_path))*0.8))\n",
    "negative_len = len(os.listdir(negative_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.scandir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/train/positive'):\n",
    "    os.remove(file.path)\n",
    "for trainimg in itertools.islice(glob.iglob(os.path.join(positive_path, '*.png')), positive_train_len):\n",
    "    shutil.copy(trainimg, 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/train/positive')\n",
    "    \n",
    "for file in os.scandir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/train/negative'):\n",
    "    os.remove(file.path)\n",
    "for trainimg in itertools.islice(glob.iglob(os.path.join(negative_path, '*.png')), negative_train_len):\n",
    "    shutil.copy(trainimg, 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/train/negative')\n",
    "\n",
    "for file in os.scandir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/test/positive'):\n",
    "    os.remove(file.path)\n",
    "for testimg in itertools.islice(glob.iglob(os.path.join(positive_path, '*.png')), positive_train_len, positive_len):\n",
    "    shutil.copy(testimg, 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/test/positive')\n",
    "\n",
    "for file in os.scandir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/test/negative'):\n",
    "    os.remove(file.path)\n",
    "for testimg in itertools.islice(glob.iglob(os.path.join(negative_path, '*.png')), negative_train_len, negative_len):\n",
    "    shutil.copy(testimg, 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\equalized_data_model/working/test/negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer\n",
    "    ]),\n",
    "    \n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer\n",
    "    ]),\n",
    "\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = {\n",
    "    'train': datasets.ImageFolder('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data/equalized_data_model/working/train', data_transforms['train']),\n",
    "    'validation': datasets.ImageFolder('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data/equalized_data_model/working/val', data_transforms['validation']),\n",
    "    'test': datasets.ImageFolder('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data/equalized_data_model/working/test', data_transforms['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data/squeezenet_model_data/working/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(data_images['train'], batch_size=32, shuffle=True, num_workers=0),\n",
    "    'validation': torch.utils.data.DataLoader(data_images['validation'], batch_size=32,shuffle=True,num_workers=0),\n",
    "    'test': torch.utils.data.DataLoader(data_images['test'], batch_size=32,shuffle=True,num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(device_id)\n",
    "# print(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\arnav/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(64, 3)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.fc.parameters())\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trained_model(model, criterion, optimizer, epochs):\n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         print('Epoch:', str(epoch+1) + '/' + str(epochs))\n",
    "#         print('-'*10)\n",
    "        \n",
    "#         for phase in ['train', 'validation']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train() #this trains the model\n",
    "#             else:\n",
    "#                 model.eval() #this evaluates the model\n",
    "\n",
    "#             running_loss, running_corrects = 0.0, 0 \n",
    "\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device) #convert inputs to cpu or cuda\n",
    "#                 labels = labels.to(device) #convert labels to cpu or cuda\n",
    "\n",
    "#                 outputs = model(inputs) #outputs is inputs being fed to the model\n",
    "#                 loss = criterion(outputs, labels) #outputs are fed into the model\n",
    "#                 # loss.requires_grad = True\n",
    "\n",
    "\n",
    "#                 if phase == 'train':\n",
    "#                     optimizer.zero_grad() #sets gradients to zero\n",
    "#                     outputs = model(inputs) #outputs is inputs being fed to the model\n",
    "#                     loss.backward() #computes sum of gradients\n",
    "#                     optimizer.step() #preforms an optimization step\n",
    "\n",
    "#                 _, preds = torch.max(outputs, 1) #max elements of outputs with output dimension of one\n",
    "#                 running_loss += loss.item() * inputs.size(0) #loss multiplied by the first dimension of inputs\n",
    "#                 running_corrects += torch.sum(preds == labels.data) #sum of all the correct predictions\n",
    "\n",
    "#             epoch_loss = running_loss / len(data_images[phase]) #this is the epoch loss\n",
    "#             epoch_accuracy = running_corrects.double() / len(data_images[phase]) #this is the epoch accuracy\n",
    "\n",
    "#             print(phase, ' loss:', epoch_loss, 'epoch_accuracy:', epoch_accuracy)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(outputs, labels):\n",
    "    op = outputs.cpu()\n",
    "    la = labels.cpu()\n",
    "    _, predss = torch.max(op, dim=1)\n",
    "    return torch.tensor(precision_score(la,predss, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            iteration_count=0\n",
    "            total_precision=0\n",
    "            total_recall=0\n",
    "            total_f1=0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # print(labels.cpu().numpy())\n",
    "                # print(\"BREAK1\")\n",
    "                # print(outputs)\n",
    "                # print(\"BREAK2\")\n",
    "                # print(_)\n",
    "                # print(\"BREAK3\")\n",
    "                # print(preds.cpu().numpy())\n",
    "                # print(\"BREAK4\")\n",
    "                # precision(labels=labels,outputs=preds)\n",
    "                # r_labels = \n",
    "                iteration_count+=1\n",
    "                precision_it= precision_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted', zero_division=0)\n",
    "                recall_it= recall_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted', zero_division=0)\n",
    "                f1_it= f1_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted', zero_division=0)\n",
    "                total_precision+=precision_it\n",
    "                total_recall+=recall_it\n",
    "                total_f1+=f1_it\n",
    "\n",
    "                # print('Precision: {}'.format(precision_it))\n",
    "                # print('total Precision: {}'.format(total_precision))\n",
    "                # print('iteration: {}'.format(iteration_count))\n",
    "                # print('Recall: {}'.format(recall_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted')))\n",
    "                # print('f1: {}'.format(f1_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted')))\n",
    "                # print('cm: {}'.format(confusion_matrix(labels.cpu().numpy(),preds.cpu().numpy())))\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_images[phase])\n",
    "            epoch_acc = running_corrects.double() / len(data_images[phase])\n",
    "            epoch_precision=total_precision/iteration_count\n",
    "            epoch_recall=total_recall/iteration_count\n",
    "            epoch_f1=total_f1/iteration_count\n",
    "\n",
    "            print('{} Loss: {:.3f} Acc: {:.3f} Prec: {:.3f} Recall: {:.3f} F1 {:.3f}'.format(\n",
    "                phase, epoch_loss, epoch_acc, epoch_precision, epoch_recall, epoch_f1))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_precision = epoch_precision\n",
    "                best_recall = epoch_recall\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best test Acc: {:.3f}, Best Precision: {:.3f}, Best Recall: {:.3f}, Best F1: {:.3f}'.format(best_acc, best_precision, best_recall, best_f1))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trained_model(model, criterion, optimizer, epochs):\n",
    "#     best_acc = 0.0\n",
    "#     best_precision = 0.0\n",
    "#     best_recall = 0.0\n",
    "#     best_f1 = 0.0\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         print('Epoch:', str(epoch+1) + '/' + str(epochs))\n",
    "#         print('-'*10)\n",
    "        \n",
    "#         for phase in ['train', 'test']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train() #this trains the model\n",
    "#             else:\n",
    "#                 model.eval() #this evaluates the model\n",
    "\n",
    "#             running_loss, running_corrects = 0.0, 0 \n",
    "\n",
    "#             iteration_count=0\n",
    "#             total_precision=0\n",
    "#             total_recall=0\n",
    "#             total_f1=0\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device) #convert inputs to cpu or cuda\n",
    "#                 labels = labels.to(device) #convert labels to cpu or cuda\n",
    "\n",
    "                \n",
    "#                 # inputs = inputs.logits\n",
    "#                 outputs = model(inputs) #outputs is inputs being fed to the model\n",
    "#                 # print('This is OUTPUT')\n",
    "#                 # print(outputs)\n",
    "#                 # outputs = outputs.logits\n",
    "#                 loss = criterion(outputs, labels) #outputs are fed into the model\n",
    "\n",
    "#                 if phase == 'train':\n",
    "#                     optimizer.zero_grad() #sets gradients to zero\n",
    "#                     loss.backward() #computes sum of gradients\n",
    "#                     optimizer.step() #preforms an optimization step\n",
    "\n",
    "#                 _, preds = torch.max(outputs, 1) #max elements of outputs with output dimension of one\n",
    "#                 running_loss += loss.item() * inputs.size(0) #loss multiplied by the first dimension of inputs\n",
    "#                 running_corrects += torch.sum(preds == labels.data) #sum of all the correct predictions\n",
    "\n",
    "#                 iteration_count+=1\n",
    "#                 precision_it= precision_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted', zero_division=0)\n",
    "#                 recall_it= recall_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted', zero_division=0)\n",
    "#                 f1_it= f1_score(labels.cpu().numpy(),preds.cpu().numpy(), average='weighted', zero_division=0)\n",
    "#                 total_precision+=precision_it\n",
    "#                 total_recall+=recall_it\n",
    "#                 total_f1+=f1_it\n",
    "\n",
    "#             epoch_loss = running_loss / len(data_images[phase]) #this is the epoch loss\n",
    "#             epoch_accuracy = running_corrects.double() / len(data_images[phase]) #this is the epoch accuracy\n",
    "#             epoch_precision=total_precision/iteration_count\n",
    "#             epoch_recall=total_recall/iteration_count\n",
    "#             epoch_f1=total_f1/iteration_count\n",
    "\n",
    "#             if phase == 'test' and epoch_accuracy > best_acc:\n",
    "#                 best_acc = epoch_accuracy\n",
    "#                 best_precision = epoch_precision\n",
    "#                 best_recall = epoch_recall\n",
    "#                 best_f1 = epoch_f1\n",
    "\n",
    "#             print('{} Loss: {:.3f} Acc: {:.3f} Prec: {:.3f} Recall: {:.3f} F1 {:.3f}'.format(\n",
    "#             phase, epoch_loss, epoch_accuracy, epoch_precision, epoch_recall, epoch_f1))\n",
    "#             # print(phase, ' loss:', epoch_loss, 'epoch_accuracy:', epoch_accuracy)\n",
    "\n",
    "\n",
    "#     print('Best test Acc: {:.3f}, Best Precision: {:.3f}, Best Recall: {:.3f}, Best F1: {:.3f}'.format(best_acc, best_precision, best_recall, best_f1))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trained_model(model, criterion, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.838 Acc: 0.588 Prec: 0.625 Recall: 0.588 F1 0.528\n",
      "test Loss: 0.885 Acc: 0.541 Prec: 0.728 Recall: 0.538 F1 0.494\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.514 Acc: 0.751 Prec: 0.805 Recall: 0.751 F1 0.728\n",
      "test Loss: 0.598 Acc: 0.614 Prec: 0.668 Recall: 0.617 F1 0.622\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.344 Acc: 0.854 Prec: 0.876 Recall: 0.854 F1 0.850\n",
      "test Loss: 0.475 Acc: 0.738 Prec: 0.769 Recall: 0.730 F1 0.731\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.308 Acc: 0.876 Prec: 0.899 Recall: 0.876 F1 0.873\n",
      "test Loss: 0.485 Acc: 0.753 Prec: 0.826 Recall: 0.752 F1 0.754\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.259 Acc: 0.904 Prec: 0.920 Recall: 0.904 F1 0.902\n",
      "test Loss: 0.829 Acc: 0.630 Prec: 0.793 Recall: 0.629 F1 0.613\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.189 Acc: 0.927 Prec: 0.936 Recall: 0.927 F1 0.927\n",
      "test Loss: 0.519 Acc: 0.774 Prec: 0.845 Recall: 0.768 F1 0.767\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.183 Acc: 0.931 Prec: 0.940 Recall: 0.931 F1 0.931\n",
      "test Loss: 0.624 Acc: 0.673 Prec: 0.819 Recall: 0.674 F1 0.660\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.103 Acc: 0.964 Prec: 0.966 Recall: 0.964 F1 0.964\n",
      "test Loss: 0.370 Acc: 0.823 Prec: 0.863 Recall: 0.820 F1 0.823\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.089 Acc: 0.970 Prec: 0.972 Recall: 0.970 F1 0.970\n",
      "test Loss: 0.599 Acc: 0.745 Prec: 0.832 Recall: 0.744 F1 0.740\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.085 Acc: 0.970 Prec: 0.972 Recall: 0.970 F1 0.970\n",
      "test Loss: 0.339 Acc: 0.837 Prec: 0.874 Recall: 0.842 F1 0.843\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.078 Acc: 0.972 Prec: 0.974 Recall: 0.972 F1 0.972\n",
      "test Loss: 1.047 Acc: 0.639 Prec: 0.815 Recall: 0.642 F1 0.625\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.070 Acc: 0.975 Prec: 0.977 Recall: 0.975 F1 0.975\n",
      "test Loss: 0.322 Acc: 0.855 Prec: 0.886 Recall: 0.855 F1 0.856\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.076 Acc: 0.972 Prec: 0.975 Recall: 0.972 F1 0.972\n",
      "test Loss: 0.388 Acc: 0.829 Prec: 0.875 Recall: 0.830 F1 0.833\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.064 Acc: 0.979 Prec: 0.980 Recall: 0.979 F1 0.979\n",
      "test Loss: 0.723 Acc: 0.734 Prec: 0.842 Recall: 0.737 F1 0.736\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.050 Acc: 0.984 Prec: 0.985 Recall: 0.984 F1 0.984\n",
      "test Loss: 0.576 Acc: 0.780 Prec: 0.859 Recall: 0.774 F1 0.776\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.049 Acc: 0.986 Prec: 0.987 Recall: 0.986 F1 0.986\n",
      "test Loss: 0.593 Acc: 0.773 Prec: 0.856 Recall: 0.771 F1 0.772\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.047 Acc: 0.987 Prec: 0.988 Recall: 0.987 F1 0.987\n",
      "test Loss: 0.613 Acc: 0.770 Prec: 0.853 Recall: 0.773 F1 0.772\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.046 Acc: 0.985 Prec: 0.986 Recall: 0.985 F1 0.985\n",
      "test Loss: 0.496 Acc: 0.811 Prec: 0.871 Recall: 0.812 F1 0.815\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.045 Acc: 0.986 Prec: 0.987 Recall: 0.986 F1 0.986\n",
      "test Loss: 0.699 Acc: 0.751 Prec: 0.852 Recall: 0.754 F1 0.754\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.045 Acc: 0.987 Prec: 0.988 Recall: 0.987 F1 0.987\n",
      "test Loss: 0.535 Acc: 0.801 Prec: 0.871 Recall: 0.803 F1 0.804\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.044 Acc: 0.987 Prec: 0.987 Recall: 0.987 F1 0.987\n",
      "test Loss: 0.649 Acc: 0.773 Prec: 0.857 Recall: 0.775 F1 0.773\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.044 Acc: 0.986 Prec: 0.987 Recall: 0.986 F1 0.986\n",
      "test Loss: 0.634 Acc: 0.774 Prec: 0.853 Recall: 0.772 F1 0.772\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.043 Acc: 0.987 Prec: 0.988 Recall: 0.987 F1 0.987\n",
      "test Loss: 0.594 Acc: 0.789 Prec: 0.865 Recall: 0.787 F1 0.790\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.041 Acc: 0.987 Prec: 0.988 Recall: 0.987 F1 0.987\n",
      "test Loss: 0.639 Acc: 0.773 Prec: 0.852 Recall: 0.775 F1 0.774\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.042 Acc: 0.988 Prec: 0.989 Recall: 0.988 F1 0.988\n",
      "test Loss: 0.633 Acc: 0.775 Prec: 0.852 Recall: 0.773 F1 0.772\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.041 Acc: 0.988 Prec: 0.988 Recall: 0.988 F1 0.988\n",
      "test Loss: 0.607 Acc: 0.783 Prec: 0.860 Recall: 0.777 F1 0.775\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.043 Acc: 0.988 Prec: 0.988 Recall: 0.988 F1 0.988\n",
      "test Loss: 0.623 Acc: 0.776 Prec: 0.864 Recall: 0.782 F1 0.784\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.042 Acc: 0.988 Prec: 0.989 Recall: 0.988 F1 0.988\n",
      "test Loss: 0.628 Acc: 0.776 Prec: 0.855 Recall: 0.770 F1 0.769\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.041 Acc: 0.988 Prec: 0.989 Recall: 0.988 F1 0.988\n",
      "test Loss: 0.623 Acc: 0.776 Prec: 0.857 Recall: 0.778 F1 0.779\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.041 Acc: 0.987 Prec: 0.987 Recall: 0.987 F1 0.987\n",
      "test Loss: 0.623 Acc: 0.776 Prec: 0.856 Recall: 0.770 F1 0.772\n",
      "\n",
      "Training complete in 28m 37s\n",
      "Best test Acc: 0.855, Best Precision: 0.886, Best Recall: 0.855, Best F1: 0.856\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# sound_file = './sound/beep.wav'\n",
    "\n",
    "# # <code that takes a long time>\n",
    "\n",
    "# Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_weights/working/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_weights/working/models/weights.h5') #save the model's weights\n",
    "model.load_state_dict(torch.load('C:\\\\Users\\\\arnav\\\\Documents\\\\Arnav\\\\Research-IITR\\\\CovidX_Github_dataset\\\\data\\\\squeezenet_model_weights/working/models/weights.h5')) #load the model's weights"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f9a49f1d600a10354e362afbc6f477ed4542a914b64ffa3b858051b01965c25"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
